{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba0410fe-99f2-4842-aef3-c59dd4caf741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Imports ==\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from qiskit_aer import Aer\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import TwoLocal\n",
    "from qiskit.quantum_info import Statevector\n",
    "from qiskit_ibm_runtime import SamplerV2 as Sampler\n",
    "from qiskit_ibm_runtime.fake_provider import FakeOsaka\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fbeccbb5-e79f-4b3d-ab1d-2ef043a249bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "\n",
    "# Standardise features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into test and training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c10b3b38-a10d-4977-9249-75f551e68639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.86168207 -1.43528754]\n",
      " [-1.59128449 -0.18399934]\n",
      " [ 0.40686002  1.22397763]\n",
      " [ 1.6349996   0.17127311]\n",
      " [ 0.02739669  0.54293623]\n",
      " [ 1.44734317 -1.60513848]\n",
      " [-0.73674419  1.43222238]\n",
      " [ 0.24203334 -1.30093751]\n",
      " [ 2.04661675  0.47562793]\n",
      " [ 1.48511023  0.01210787]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63feae6f-4af6-44d0-ab61-d835908e0704",
   "metadata": {},
   "source": [
    "These is pretty simple data so we can angularly encode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "568d36eb-797d-4aed-a1d9-ffdd8bbe3460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x: np.ndarray) -> QuantumCircuit:\n",
    "    qc = QuantumCircuit(len(x))\n",
    "    for i in range(len(x)):\n",
    "        qc.ry(x[i], i)\n",
    "    return qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "473251a3-8697-4f3b-818e-e8fc54866894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational Quantum Classifier\n",
    "def vqc(reps: int, n_qubits: int) -> QuantumCircuit:\n",
    "    # Build ansatz with alternating rotation blocks\n",
    "    ansatz = TwoLocal(n_qubits, rotation_blocks=['ry', 'rx'], entanglement_blocks='cz',\n",
    "                          entanglement='linear', reps=reps, insert_barriers=True)\n",
    "\n",
    "    return ansatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "017b81c6-917b-4643-baad-9f2ee5fc0fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, theta_vals):\n",
    "    # Encode data\n",
    "    qc = encode(x)\n",
    "    \n",
    "    # Assign params to ansatz and combine\n",
    "    bound_ansatz = ansatz.assign_parameters(theta_vals)\n",
    "    qc.compose(bound_ansatz, inplace=True)\n",
    "    \n",
    "    # Get statevector\n",
    "    sv = Statevector.from_instruction(qc)\n",
    "    \n",
    "    # Compute probability of measuring |1> on qubit 0\n",
    "    probs = sv.probabilities_dict()\n",
    "    prob_1 = sum(v for k, v in probs.items() if k[-1] == '1')  # check last bit\n",
    "    return prob_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9eb481c5-7b0a-4248-b1d1-37b66caad444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe update this, this could be better\n",
    "def loss_fn(X, y, theta_vals):\n",
    "    loss = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        prob = predict(xi, theta_vals)\n",
    "        pred = prob # Predicted probability of class 1 (don't need deepcopy here because it's a float)\n",
    "        loss += (pred - yi) ** 2\n",
    "    return loss / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de7d7054-abb4-45de-9410-6e08adc3a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise problem circuit\n",
    "n_qubits = 2\n",
    "reps = 3\n",
    "ansatz = vqc(reps, n_qubits)\n",
    "param_vector = ParameterVector('Î¸', length=len(ansatz.parameters))\n",
    "\n",
    "# Assign parameter vector to ansatz\n",
    "param_dict = dict(zip(ansatz.parameters, param_vector))\n",
    "ansatz.assign_parameters(param_dict, inplace=True)\n",
    "\n",
    "# Simulator\n",
    "sampler = Aer.get_backend('statevector_simulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9f370ab8-7895-46f8-91cc-cbd663c724cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00: Loss = 0.2507\n",
      "Epoch 01: Loss = 0.2417\n",
      "Epoch 02: Loss = 0.2330\n",
      "Epoch 03: Loss = 0.2246\n",
      "Epoch 04: Loss = 0.2167\n",
      "Epoch 05: Loss = 0.2091\n",
      "Epoch 06: Loss = 0.2019\n",
      "Epoch 07: Loss = 0.1951\n",
      "Epoch 08: Loss = 0.1888\n",
      "Epoch 09: Loss = 0.1828\n",
      "Epoch 10: Loss = 0.1773\n",
      "Epoch 11: Loss = 0.1721\n",
      "Epoch 12: Loss = 0.1672\n",
      "Epoch 13: Loss = 0.1628\n",
      "Epoch 14: Loss = 0.1586\n",
      "Epoch 15: Loss = 0.1548\n",
      "Epoch 16: Loss = 0.1512\n",
      "Epoch 17: Loss = 0.1480\n",
      "Epoch 18: Loss = 0.1449\n",
      "Epoch 19: Loss = 0.1421\n",
      "Epoch 20: Loss = 0.1396\n",
      "Epoch 21: Loss = 0.1372\n",
      "Epoch 22: Loss = 0.1350\n",
      "Epoch 23: Loss = 0.1330\n",
      "Epoch 24: Loss = 0.1311\n",
      "Epoch 25: Loss = 0.1294\n",
      "Epoch 26: Loss = 0.1278\n",
      "Epoch 27: Loss = 0.1263\n",
      "Epoch 28: Loss = 0.1250\n",
      "Epoch 29: Loss = 0.1237\n",
      "Epoch 30: Loss = 0.1225\n",
      "Epoch 31: Loss = 0.1215\n",
      "Epoch 32: Loss = 0.1205\n",
      "Epoch 33: Loss = 0.1195\n",
      "Epoch 34: Loss = 0.1187\n",
      "Epoch 35: Loss = 0.1178\n",
      "Epoch 36: Loss = 0.1171\n",
      "Epoch 37: Loss = 0.1164\n",
      "Epoch 38: Loss = 0.1157\n",
      "Epoch 39: Loss = 0.1151\n",
      "Epoch 40: Loss = 0.1145\n",
      "Epoch 41: Loss = 0.1140\n",
      "Epoch 42: Loss = 0.1135\n",
      "Epoch 43: Loss = 0.1130\n",
      "Epoch 44: Loss = 0.1126\n",
      "Epoch 45: Loss = 0.1121\n",
      "Epoch 46: Loss = 0.1118\n",
      "Epoch 47: Loss = 0.1114\n",
      "Epoch 48: Loss = 0.1110\n",
      "Epoch 49: Loss = 0.1107\n",
      "Epoch 50: Loss = 0.1104\n",
      "Epoch 51: Loss = 0.1101\n",
      "Epoch 52: Loss = 0.1098\n",
      "Epoch 53: Loss = 0.1095\n",
      "Epoch 54: Loss = 0.1093\n",
      "Epoch 55: Loss = 0.1090\n",
      "Epoch 56: Loss = 0.1088\n",
      "Epoch 57: Loss = 0.1086\n",
      "Epoch 58: Loss = 0.1084\n",
      "Epoch 59: Loss = 0.1082\n",
      "Epoch 60: Loss = 0.1080\n",
      "Epoch 61: Loss = 0.1078\n",
      "Epoch 62: Loss = 0.1076\n",
      "Epoch 63: Loss = 0.1074\n",
      "Epoch 64: Loss = 0.1073\n",
      "Epoch 65: Loss = 0.1071\n",
      "Epoch 66: Loss = 0.1070\n",
      "Epoch 67: Loss = 0.1068\n",
      "Epoch 68: Loss = 0.1067\n",
      "Epoch 69: Loss = 0.1066\n",
      "Epoch 70: Loss = 0.1065\n",
      "Epoch 71: Loss = 0.1063\n",
      "Epoch 72: Loss = 0.1062\n",
      "Epoch 73: Loss = 0.1061\n",
      "Epoch 74: Loss = 0.1060\n",
      "Epoch 75: Loss = 0.1059\n",
      "Epoch 76: Loss = 0.1058\n",
      "Epoch 77: Loss = 0.1057\n",
      "Epoch 78: Loss = 0.1056\n",
      "Epoch 79: Loss = 0.1055\n",
      "Epoch 80: Loss = 0.1054\n",
      "Epoch 81: Loss = 0.1054\n",
      "Epoch 82: Loss = 0.1053\n",
      "Epoch 83: Loss = 0.1052\n",
      "Epoch 84: Loss = 0.1051\n",
      "Epoch 85: Loss = 0.1051\n",
      "Epoch 86: Loss = 0.1050\n",
      "Epoch 87: Loss = 0.1049\n",
      "Epoch 88: Loss = 0.1049\n",
      "Epoch 89: Loss = 0.1048\n",
      "Epoch 90: Loss = 0.1047\n",
      "Epoch 91: Loss = 0.1047\n",
      "Epoch 92: Loss = 0.1046\n",
      "Epoch 93: Loss = 0.1046\n",
      "Epoch 94: Loss = 0.1045\n",
      "Epoch 95: Loss = 0.1044\n",
      "Epoch 96: Loss = 0.1044\n",
      "Epoch 97: Loss = 0.1043\n",
      "Epoch 98: Loss = 0.1043\n",
      "Epoch 99: Loss = 0.1043\n"
     ]
    }
   ],
   "source": [
    "# == TRAIN ==\n",
    "theta = np.random.uniform(0, 2 * np.pi, len(param_vector))\n",
    "lr = 0.1\n",
    "for epoch in range(100):\n",
    "    grad = np.zeros_like(theta)\n",
    "    eps = 1e-4\n",
    "    # Finite difference gradient\n",
    "    for i in range(len(theta)):\n",
    "        theta_plus = theta.copy()\n",
    "        theta_plus[i] += eps\n",
    "        theta_minus = theta.copy()\n",
    "        theta_minus[i] -= eps\n",
    "\n",
    "        grad[i] = (loss_fn(X_train, y_train, theta_plus) - loss_fn(X_train, y_train, theta_minus)) / (2 * eps)\n",
    "\n",
    "    theta -= lr * grad\n",
    "    train_loss = loss_fn(X_train, y_train, theta)\n",
    "    print(f\"Epoch {epoch:02d}: Loss = {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f73fc-1837-47db-90b4-e6352619080c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qiskit (stable)",
   "language": "python",
   "name": "qiskit-stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
