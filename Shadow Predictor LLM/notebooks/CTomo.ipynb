{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8da45b-0388-44b2-92c6-dd3cbebd8623",
   "metadata": {},
   "source": [
    "# Classical Shadow Tomography\n",
    "\n",
    "This notebook aims to introduce theory of Classical Shadow Tomography (CST) methods, following [Aaronson's](https://arxiv.org/abs/1711.01053) (2018) original (shadow tomography) approach with [Huang et al.'s](https://arxiv.org/abs/2002.08953) (2020) improvements (CST).\n",
    "\n",
    "The overarching theory for Shadow Tomogrpahy is stated as follows from Aaronson:\n",
    "\n",
    "> **Theorem 1 (Shadow Tomography; Aaronson, 2018)**\n",
    ">\n",
    "> Let $\\rho$ be an unknown $D$-dimensional mixed state, and let $E_1,\\dots,E_M$ be known two-outcome measurements.\n",
    ">\n",
    "> Then, for any $\\varepsilon,\\delta>0$, there exists a procedure that estimates\n",
    "> $\\Pr[E_i \\text{ accepts } \\rho]$ to within $\\pm\\varepsilon$ for all $i\\in[M]$, with success probability at least $1-\\delta$, using\n",
    ">\n",
    "> $$\\widetilde{O}\\!\\left(\\frac{\\log^4 M \\cdot \\log D}{\\varepsilon^5}\\right)$$\n",
    ">\n",
    "> copies of $\\rho$.\n",
    "\n",
    "The general gist of this theorem implies that, we can measure $k = \\mathcal{O}(...)$ copies of state $U \\rho U^{\\textdagger}$, to reconstruct the state with the 2 outcome measurements in the computational basis. These unitary bases are drawn from a random ensemble; covered in [Huang et al.](https://arxiv.org/abs/2002.08953) (2020) and this literature review from [Tufts](https://www.cs.tufts.edu/comp/150QC/Report1Mingqian.pdf). The unitary and measurement operations are applied to $\\rho$ a number of times which determines the size of the classical shadow. From this we obtain observables, which are representations of expected outputs from $\\rho$. These are formed form the classical shadow and are what we want to predict, as they will provide us with useful information of quantum properties of our state. Observables can be any operator project, combination of paulis, or full hamiltonians.\n",
    "\n",
    "The theorem for CST is as follows:\n",
    "> **Theorem 2 (Classical Shadows; Huang–Kueng–Preskill, 2020)**\n",
    ">\n",
    "> Let $\\rho$ be an unknown $n$-qubit state. Fix a randomized measurement scheme that admits a classical-shadow estimator (e.g., independently sample a single-qubit Pauli/Clifford on each qubit, apply it to $\\rho$, and classically invert the channel).  \n",
    "> Given $N$ independent snapshots, there exists an estimator that outputs\n",
    "> $\\{\\widehat{\\mu}_i\\}_{i=1}^M$ for $\\mu_i=\\operatorname{Tr}(O_i\\rho)$ such that, with probability at least $1-\\delta$,\n",
    "> all $M$ estimates are $\\varepsilon$-accurate simultaneously:\n",
    "> $|\\widehat{\\mu}_i-\\mu_i|\\le \\varepsilon$ for every $i\\in[M]$, provided\n",
    ">\n",
    "> $$N \\;\\ge\\; C\\,\\frac{\\max_{i\\in[M]}\\,\\|O_i\\|_{\\mathrm{sh}}^{2}\\;\\log(M/\\delta)}{\\varepsilon^{2}},$$\n",
    ">\n",
    "> where $\\|O\\|_{\\mathrm{sh}}$ is the shadow norm (variance proxy) determined by the chosen measurement ensemble and $C$ is a universal constant.\n",
    ">\n",
    "> **In particular:**\n",
    "> - For independent random single-qubit Pauli measurements and any $k$-local observable $O$ with $\\|O\\|\\le 1$, one has $\\|O\\|_{\\mathrm{sh}}^{2}\\!\\le 3^{k}$, so\n",
    ">   $$N = O\\!\\left(\\frac{3^{k}\\,\\log(M/\\delta)}{\\varepsilon^{2}}\\right).$$\n",
    "> - For fidelity (rank-1 projector) observables $O=\\lvert\\psi\\rangle\\!\\langle\\psi\\rvert$, one has $\\|O\\|_{\\mathrm{sh}}^{2}\\!\\le 2^{n}$, so\n",
    ">   $$N = O\\!\\left(\\frac{2^{n}\\,\\log(M/\\delta)}{\\varepsilon^{2}}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e14b9-14d4-4e74-ab92-d0faecefcc95",
   "metadata": {},
   "source": [
    "## Recreating Classical Shadow Tomography\n",
    "In order to recreate CST, the first thing we need to do is set up our Clifford unitary basis operation. This can be done easily with Qiskit's built-in function. You can read the documentation for the Clifford gate from Qiskit [here](https://quantum.cloud.ibm.com/docs/en/api/qiskit/qiskit.quantum_info.Clifford). These can be created on the fly when running the full quantum circuit, so we will implement everything as one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "048e6f70-963e-4538-9167-ab04cdee77bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "from CShadTomo import CTomo, ShadowTomoPlotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d5b7d8-dc9a-47ce-b13b-7acd344d7e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `ToSProject` has been saved to /Users/zacsmms/.cache/huggingface/stored_tokens\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 61, in main\n",
      "    service.run()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/commands/user.py\", line 113, in run\n",
      "    login(\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/_login.py\", line 126, in login\n",
      "    _login(token, add_to_git_credential=add_to_git_credential)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/_login.py\", line 412, in _login\n",
      "    _set_active_token(token_name=token_name, add_to_git_credential=add_to_git_credential)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/_login.py\", line 457, in _set_active_token\n",
      "    raise ValueError(f\"Token {token_name} not found in {constants.HF_STORED_TOKENS_PATH}\")\n",
      "ValueError: Token ToSProject not found in /Users/zacsmms/.cache/huggingface/stored_tokens\n"
     ]
    }
   ],
   "source": [
    "# Set HuggingFace Token\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"\"\n",
    "\n",
    "!huggingface-cli login --token \"$HF_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3903ed67-6f88-4884-bbdd-0735e7771361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZI: mean=0.0294, SE=0.0246\n",
      "IZ: mean=0.0114, SE=0.0247\n",
      "XX: mean=0.9558, SE=0.0392\n",
      "ZZ: mean=1.0422, SE=0.0407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d28f25b709945aea1fe8b98876815cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb739bb7d423482e9154286ad4bba71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51f40d50d7a4509bcdaaf3e89689d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8311dc65f08a4740ad0b3c5f6ef8f3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94128e8381534308ae0ff9765e74722a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b83a422e7d14cbaa10e1f2237c12128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e228fa4f204ae591d86d90ded0b8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "# ---- driver.py (example usage) ----\n",
    "import numpy as np\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit import QuantumCircuit\n",
    "\n",
    "# For set_llm() to work:\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# 1) Define your state-prep routine\n",
    "def prep_bell(qc: QuantumCircuit):\n",
    "    qc.h(0)\n",
    "    qc.cx(0, 1)\n",
    "    return qc  # optional, class doesn't use the return value\n",
    "\n",
    "# 2) Instantiate tomography (choose a scheme)\n",
    "tomo = CTomo(\n",
    "    n_qubits=2,\n",
    "    scheme=\"local-clifford\",          # or \"local-pauli\" / \"global-clifford\"\n",
    "    num_snapshots=5000,\n",
    "    default_predictor=\"classical\",     # or \"llm\"\n",
    ")\n",
    "\n",
    "# 3) Backend + state-prep + observables\n",
    "tomo.set_backend(AerSimulator())       # or an IBM backend name if using Runtime\n",
    "tomo.set_state_prep(prep_bell)\n",
    "tomo.set_observables([\"ZI\", \"IZ\", \"XX\", \"ZZ\"])\n",
    "\n",
    "# 4) Run shots (pick one)\n",
    "tomo.run_snapshots()                   # runs exactly num_snapshots\n",
    "# or, e.g., cap by time/shots and auto-save checkpoints:\n",
    "# tomo.run_snapshots_capped(N=10000, max_seconds_total=60, save_every=1000, save_path=\"data/snapshots.json\")\n",
    "\n",
    "# (Optional) Save the snapshot log to file for resuming later\n",
    "tomo.save_now(\"data/snapshots.json\")\n",
    "\n",
    "# 5) Classical estimates (mean ± SE) for each observable\n",
    "for O in tomo.observables:\n",
    "    mean, se = tomo.estimate_observable_classical(O)\n",
    "    print(f\"{O:>2}: mean={mean:.4f}, SE={se:.4f}\")\n",
    "\n",
    "# 6) Save per-snapshot contributions to .npz (for plotting later)\n",
    "#    Do this for whichever observable(s) you want hist/running-mean plots for.\n",
    "tomo.save_results_classical(\"ZZ\", \"out/classical_ZZ.npz\")\n",
    "\n",
    "# 7) (Optional) LLM setup + values\n",
    "#    Make sure your GPU/CPU can handle the model you pick.\n",
    "#    Comment these lines out if you’re not using the LLM path.\n",
    "tomo.set_llm(\"Qwen/Qwen2.5-1.5B-Instruct\")  # example HF model\n",
    "mean_llm, conf_llm = tomo.estimate_observable_llm(\"ZZ\")\n",
    "print(f\"LLM(ZZ): mean={mean_llm:.4f}, avg_conf={conf_llm:.3f}\")\n",
    "tomo.save_results_llm(\"ZZ\", \"out/llm_ZZ.npz\")\n",
    "\n",
    "# 8) Plotting (vector outputs). Create a plotter attached to your tomo.\n",
    "plotter = ShadowTomoPlotter(tomo)\n",
    "\n",
    "# Individual vector plots (SVGs)\n",
    "plotter.plot_running_mean_from_file(\"out/classical_ZZ.npz\", save_dir=\"plots\")\n",
    "plotter.plot_hist_from_file(\"out/classical_ZZ.npz\", save_dir=\"plots\")\n",
    "plotter.plot_bit_marginals(save_dir=\"plots\")\n",
    "plotter.plot_basis_usage(save_dir=\"plots\")\n",
    "plotter.plot_stats_dashboard([\"ZI\", \"IZ\", \"XX\", \"ZZ\"], save_dir=\"plots\")\n",
    "# If you created LLM results:\n",
    "plotter.plot_llm_confidence_from_file(\"out/llm_ZZ.npz\", save_dir=\"plots\")\n",
    "plotter.plot_llm_vs_classical([\"ZI\", \"IZ\", \"XX\", \"ZZ\"], save_dir=\"plots\")\n",
    "\n",
    "# One combined multi-panel vector figure (also SVG)\n",
    "plotter.plot_all_vector(\n",
    "    save_dir=\"plots\",\n",
    "    classical_path=\"out/classical_ZZ.npz\",\n",
    "    llm_path=\"out/llm_ZZ.npz\",             # omit if you didn’t run LLM\n",
    "    compare_observables=[\"ZI\", \"IZ\", \"XX\", \"ZZ\"],\n",
    "    stats_observables=[\"ZI\", \"IZ\", \"XX\", \"ZZ\"],\n",
    "    filename=\"all_plots.svg\"\n",
    ")\n",
    "\n",
    "# 9) Resuming later (optional):\n",
    "# tomo.resume_snapshots_capped(N=5000, load_path=\"data/snapshots.json\",\n",
    "#                              save_path=\"data/snapshots.json\",\n",
    "#                              max_seconds_total=60, save_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f20ab-d969-430e-8f74-ec80c5772225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qiskit (stable)",
   "language": "python",
   "name": "qiskit-stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
