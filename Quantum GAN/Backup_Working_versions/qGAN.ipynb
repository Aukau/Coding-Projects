{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a27609e2-7051-44b0-a7e9-0c08754aa464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.quantum_info import Statevector\n",
    "from qiskit.circuit.library import Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627a2e5-4929-4790-89e1-6de64be93735",
   "metadata": {},
   "source": [
    "# Quantum Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d99887-09d4-42c4-bb48-4c464b8db819",
   "metadata": {},
   "source": [
    "This notebook will aim to implement a quantum generator and classical discriminator to check the probability distribution of classical training samples.\n",
    "\n",
    "Note: This will be based off of work provided in Nature from Zoufal et al. (2019)\n",
    "\n",
    "The quantum generator is trained to transform an input state into an output state, based on the following,\n",
    "\n",
    "\\begin{equation}\n",
    "G_{\\theta} |\\psi_{\\text{in}}\\rangle = g_{\\theta} = \\sum_{j = 0}^{2^n - 1} \\sqrt{p_{\\theta}^j} \\, |j\\rangle\n",
    "\\end{equation}\n",
    "\n",
    "p is the occurrence probabilities of basis state j. The sample space X is from {0, ..., 2^n-1}.\n",
    "\n",
    "The qGen is implemented in a variational form, a parametrised quantum circuit. The k layers consist of alternating rotation-Y block with entanglement blocks U_ent. The rotations are parametrised by theta^(i, j) for ith qubit in the jth layer. for n qubits the variational circuit uses (k+1)n parametrised single qubit gates, and kn two-qubits gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4d5ba1d-c504-497d-a4ba-0a253086b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum Generator circuit\n",
    "def gen(theta, n, k):\n",
    "    \"\"\"\n",
    "        This function generates the quantum generator circuit for n qubits. It takes in\n",
    "        parameters theta in an array format.\n",
    "    \"\"\"\n",
    "    qc = QuantumCircuit(n)\n",
    "\n",
    "    for j in range(k):\n",
    "        # Rotation block\n",
    "        for i in range(n):\n",
    "            qc.ry(theta[i, j], i)\n",
    "\n",
    "        # Entanglement block\n",
    "        if j != k - 1:\n",
    "            for i in range(n - 1):\n",
    "                qc.cz(i, i+1)\n",
    "            qc.cz(n - 1, 0)\n",
    "\n",
    "    return qc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c87be-672c-426f-879c-fac766d9565b",
   "metadata": {},
   "source": [
    "We now need to calculate the loss functions, given m data samples and g^l from the quantum generator. With m randomly chosen training data samples x^l for l = 1, ..., m the loss functions are:\n",
    "\n",
    "For the generator:\n",
    "\\begin{equation}\n",
    "L_G (\\phi, \\theta) = - \\frac{1}{m} \\sum^m_{l=1} \\left[ log D_{\\phi} (g^l) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "For the discriminator (standard binary cross-entropy loss):\n",
    "\\begin{equation}\n",
    "L_D (\\phi, \\theta) = - \\frac{1}{m} \\sum^m_{l=1} \\left[ log D_{\\phi} (x^l) + log(1 - D_{\\phi} (g^l)) \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424542a4-7657-487d-98f0-c2512be2e1fc",
   "metadata": {},
   "source": [
    "## Generator input state preparation\n",
    "We create a normal distribution with $\\mu$ and $\\sigma$ being the estimates of mean and standard deviation of training data samples. So we need to figure out how to create a normal distribution for the input state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba2bef70-076a-4651-b0bd-e58f203e4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is included in the finance functions\n",
    "from qiskit_finance.circuit.library.probability_distributions import NormalDistribution as ND\n",
    "\n",
    "def norm_distr(n, mu, sigma, bounds):\n",
    "    \"\"\"\n",
    "        Generates a normal distribution based on the parameters provided.\n",
    "    \"\"\"\n",
    "    qc = ND(n, mu=mu, sigma=sigma, bounds=bounds)\n",
    "    return qc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c720ec4-8440-489a-a147-cc69eacf1780",
   "metadata": {},
   "source": [
    "The thing to know with this is that the normaldistribution is not efficiently found for a quantum state. So this function estimates the likely normal distribution based on the parameters. We're basically abusing the difficulty in generation by using prebuilt functions in qiskit, but it's not too bad otherwise, just more code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0407afaa-7f89-4e81-bf0a-058e060150b4",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "This is the classical implementation of our qGAN, it will consist of a 50-node input layer, a 20-node hidden layer, and a single-node output layer. We'll use linear transforms followed by leaky ReLUs (maybe GELU depending on whether it works or not), then a linear transform at the output layer and a sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91dc5b5f-17c2-4ac7-bbae-eea6f0e88655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e365e2b4-d06e-4c30-8658-5ece5952cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "    def __init__(self, input_size=50, hidden_size=20, output_size=1):\n",
    "        super(discriminator, self).__init__()\n",
    "\n",
    "        # First linear layer\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Leaky relu\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "\n",
    "        # Second linear layer\n",
    "        self.l2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First linear layer\n",
    "        x = self.l1(x)\n",
    "        \n",
    "        # Apply leaky ReLU\n",
    "        x = self.leaky_relu(self.l1(x))\n",
    "\n",
    "        # Apply second layer\n",
    "        x = self.l2(x)\n",
    "\n",
    "        # Sigmoid activation\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "161a4826-31e8-46c1-af41-5c665e3a97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also just use a sequential approach given it's probably easier\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(50, 20),\n",
    "    nn.LeakyReLU(0.01),\n",
    "    nn.Linear(20, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a66a7da-99af-4ccc-8984-f4b9b04fe064",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "The qGAN is trained using AMSGrad with an inital learning rate of 10^-4. The stability of the training is improved with a gradient penalty on the discriminator's loss function.\n",
    "\n",
    "Training data is split into batches of 2000.\n",
    "\n",
    "The generated data samples are created by preparing and measuring the qGen 2000 times, the batches are then used to update the parameters of the discriminator and the generator in an alternating fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d665df8a-e72a-4b3c-8c9e-86bb6c88e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of parameters (n x k)\n",
    "n = 4\n",
    "k = 8\n",
    "\n",
    "# Generator parameters\n",
    "gen_params = nn.Parameter(torch.randn(n, k, requires_grad=True))\n",
    "\n",
    "# Generator optimiser\n",
    "gen_opt = optim.Adam([gen_params], lr=10**(-4), amsgrad=True)\n",
    "\n",
    "# Discriminator optimiser\n",
    "dis_opt = optim.Adam(model.parameters(), lr=10**(-4), amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a83bc214-1ada-49a0-a167-97325e873e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qgan(\n",
    "    quantum_generator_fn, generator_params, discriminator, dataloader, num_epochs=100,\n",
    "    disc_lr=1e-4, latent_dim=8, device='cpu', generator_update_fn=None\n",
    "):\n",
    "\n",
    "    discriminator.to(device)\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    dis_opt = optim.Adam(discriminator.parameters(), lr=disc_lr, amsgrad=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        for real_data_batch in tqdm(dataloader, desc=\"Training step\"):\n",
    "            real_data = real_data_batch[0].to(device)\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # === Train Discriminator ===\n",
    "            dis_opt.zero_grad()\n",
    "\n",
    "            # Real data\n",
    "            real_preds = discriminator(real_data)\n",
    "            real_loss = torch.log(real_preds + epsilon)\n",
    "\n",
    "            # Fake data (from quantum circuit)\n",
    "            noise = torch.randn(batch_size, latent_dim)\n",
    "            fake_data = quantum_generator_fn(noise, generator_params).detach().to(device)\n",
    "            fake_preds = discriminator(fake_data)\n",
    "            fake_loss = torch.log(1 - fake_preds + epsilon)\n",
    "\n",
    "            disc_loss = -torch.mean(real_loss + fake_loss)\n",
    "            disc_loss.backward()\n",
    "            dis_opt.step()\n",
    "\n",
    "            # === Train Generator ===\n",
    "            # Do NOT use .backward() — assume manual gradient update\n",
    "            noise = torch.randn(batch_size, latent_dim)\n",
    "            fake_data = quantum_generator_fn(noise, generator_params).to(device)\n",
    "            fake_preds = discriminator(fake_data)\n",
    "\n",
    "            gen_loss = -torch.mean(torch.log(fake_preds + epsilon))\n",
    "\n",
    "            generator_update_fn(generator_params, gen_loss, noise, discriminator)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]  D_loss: {disc_loss.item():.4f}  G_loss: {gen_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d041c2-9fbd-4a3b-a910-05fa3ee7f234",
   "metadata": {},
   "source": [
    "### Updating the qGenerator\n",
    "Our quantum generator is insufficient in its current capacity for the training cycles, so we need to update it based on what our training function does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0386a15-0944-4348-bfc5-e23a20c8f625",
   "metadata": {},
   "source": [
    "#### Adding an extra bit in here to allow for a normal encoding that doesn't blow up in size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3126feed-1f8f-434d-b821-6b3262c6ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_normal_encoding(n, mu=0, sigma=1, bounds=(-np.pi, np.pi)):\n",
    "    dim = 2 ** n\n",
    "    x = np.linspace(bounds[0], bounds[1], dim)\n",
    "\n",
    "    # Evaluate normal distribution (PDF)\n",
    "    probs = np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "    probs /= np.sum(probs)  # Normalize\n",
    "\n",
    "    # Convert to amplitudes\n",
    "    amps = np.sqrt(probs)\n",
    "\n",
    "    # Create quantum circuit\n",
    "    qc = QuantumCircuit(n)\n",
    "    init = Initialize(amps)\n",
    "    qc.append(init, range(n))\n",
    "\n",
    "    return qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36a759b0-fde7-404e-9ae3-16092ee3c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_aer.primitives import Sampler\n",
    "\n",
    "def quantum_gen_fn(noise_batch, theta, n, k, bounds, mu, sigma):\n",
    "    sampler = Sampler()\n",
    "    circuits = []\n",
    "\n",
    "    for noise_vec in noise_batch:\n",
    "        # Convert theta to np\n",
    "        theta_np = theta.detach().cpu().numpy()\n",
    "\n",
    "        # Build quantum circuit with noise encoding\n",
    "        qc = QuantumCircuit(n)\n",
    "        qc = custom_normal_encoding(n, mu=mu, sigma=sigma, bounds=bounds)\n",
    "\n",
    "        # Apply rotation + entanglement blocks\n",
    "        for j in range(k):\n",
    "            for i in range(n):\n",
    "                qc.ry(theta_np[i, j], i)\n",
    "            if j != k - 1:\n",
    "                for i in range(n - 1):\n",
    "                    qc.cz(i, i+1)\n",
    "                qc.cz(n - 1, 0)\n",
    "\n",
    "        qc.measure_all()\n",
    "        circuits.append(qc)\n",
    "\n",
    "    results = sampler.run(circuits).result()\n",
    "\n",
    "    # Extract probabilities\n",
    "    batch_outputs = []\n",
    "    for i, result in enumerate(results.quasi_dists):\n",
    "        probs = np.zeros(n)\n",
    "        for bitstring, prob in result.items():\n",
    "            bits = [int(b) for b in format(int(bitstring), f'0{n}b')]\n",
    "            probs += np.array(bits) * prob\n",
    "        batch_outputs.append(probs)\n",
    "\n",
    "    return torch.tensor(batch_outputs, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad41b37f-01f2-4914-9183-464e8ebb9c88",
   "metadata": {},
   "source": [
    "# Gradient update function\n",
    "Now we want the function that updates the gradient for the generator. In the supplementary materials of the paper, they detailed how they update parameters, this is pretty complex so we'll just accept it and hope it works out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13990757-b4ee-48c8-b284-fa4cf1a1b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_shift_update(theta, noise_batch, discriminator, lr, n_qubits, k_layers, bounds, mu, sigma):\n",
    "    n, k = theta.shape\n",
    "    batch_size = noise_batch.size(0)\n",
    "    shift = np.pi / 2\n",
    "\n",
    "    grad = torch.zeros_like(theta)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            for j in range(k):\n",
    "                # Shifted parameter sets\n",
    "                theta_plus = theta.clone()\n",
    "                theta_plus[i, j] += shift\n",
    "                theta_minus = theta.clone()\n",
    "                theta_minus[i, j] -= shift\n",
    "\n",
    "                # Generate samples using shifted parameters\n",
    "                g_plus = quantum_gen_fn(noise_batch, theta_plus, n_qubits, k_layers, bounds, mu, sigma)\n",
    "                g_minus = quantum_gen_fn(noise_batch, theta_minus, n_qubits, k_layers, bounds, mu, sigma)\n",
    "\n",
    "                # Evaluate discriminator outputs\n",
    "                D_plus = discriminator(g_plus)\n",
    "                D_minus = discriminator(g_minus)\n",
    "\n",
    "                # Compute gradient: average over batch\n",
    "                log_d_plus = torch.log(D_plus + 1e-8)\n",
    "                log_d_minus = torch.log(D_minus + 1e-8)\n",
    "                grad[i, j] = -torch.mean((log_d_plus - log_d_minus) / 2)\n",
    "\n",
    "        # Gradient descent step\n",
    "        theta -= lr * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cd12f1-c6e3-4411-ae11-0e9e9b9bc4f4",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "Because I haven't gotten a database of information we want, we'll ignore that detail for the time being and search later on for something to apply this to. Our data will have batches of 2000 items, so we'll use that to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9be2660e-75db-4e20-9a85-331d6761bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def train_loader(real_data=None, num_samples = 2000, feature_dim=50, batch_size=32, shuffle=True):\n",
    "    if real_data is None:\n",
    "        real_data = torch.randn(num_samples, feature_dim)\n",
    "\n",
    "    dataset = TensorDataset(real_data)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2f343e8-c000-4aca-aded-ca8f93c25de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step:   0%|                                     | 0/32 [00:00<?, ?it/s]/var/folders/9v/3k54msls5_lcsl4_4qdlbhn40000gn/T/ipykernel_26354/3927006739.py:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  return torch.tensor(batch_outputs, dtype=torch.float32)\n",
      "Training step: 100%|████████████████████████████| 32/32 [00:30<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]  D_loss: 1.4039  G_loss: 0.8008\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:30<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100]  D_loss: 1.3809  G_loss: 0.8005\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:30<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100]  D_loss: 1.3594  G_loss: 0.7998\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:29<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100]  D_loss: 1.3662  G_loss: 0.8003\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:29<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100]  D_loss: 1.3143  G_loss: 0.7990\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:29<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100]  D_loss: 1.2895  G_loss: 0.7990\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:30<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100]  D_loss: 1.3637  G_loss: 0.7988\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:33<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100]  D_loss: 1.3415  G_loss: 0.7983\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:36<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100]  D_loss: 1.2818  G_loss: 0.7988\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:38<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100]  D_loss: 1.2957  G_loss: 0.7993\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:38<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100]  D_loss: 1.2992  G_loss: 0.7993\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:40<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100]  D_loss: 1.3099  G_loss: 0.7995\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:40<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100]  D_loss: 1.2691  G_loss: 0.8004\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:39<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100]  D_loss: 1.2969  G_loss: 0.8022\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:39<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100]  D_loss: 1.1818  G_loss: 0.8031\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:40<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100]  D_loss: 1.1931  G_loss: 0.8051\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:44<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100]  D_loss: 1.1860  G_loss: 0.8055\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:46<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100]  D_loss: 1.2813  G_loss: 0.8081\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:43<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100]  D_loss: 1.2443  G_loss: 0.8093\n",
      "Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:43<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100]  D_loss: 1.2262  G_loss: 0.8114\n",
      "Epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:45<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100]  D_loss: 1.1703  G_loss: 0.8131\n",
      "Epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:43<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100]  D_loss: 1.1951  G_loss: 0.8176\n",
      "Epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:44<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100]  D_loss: 1.1455  G_loss: 0.8203\n",
      "Epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:45<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100]  D_loss: 1.1404  G_loss: 0.8232\n",
      "Epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:45<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100]  D_loss: 1.1956  G_loss: 0.8261\n",
      "Epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:46<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100]  D_loss: 1.2124  G_loss: 0.8294\n",
      "Epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:47<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100]  D_loss: 1.1916  G_loss: 0.8331\n",
      "Epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:47<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100]  D_loss: 1.1203  G_loss: 0.8355\n",
      "Epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:48<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100]  D_loss: 1.1344  G_loss: 0.8396\n",
      "Epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:48<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100]  D_loss: 1.0622  G_loss: 0.8428\n",
      "Epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:48<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100]  D_loss: 1.0884  G_loss: 0.8462\n",
      "Epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:49<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100]  D_loss: 1.2270  G_loss: 0.8494\n",
      "Epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:49<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100]  D_loss: 1.1502  G_loss: 0.8531\n",
      "Epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:50<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100]  D_loss: 0.9822  G_loss: 0.8550\n",
      "Epoch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:49<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100]  D_loss: 1.1234  G_loss: 0.8613\n",
      "Epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:51<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100]  D_loss: 1.0230  G_loss: 0.8651\n",
      "Epoch 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:51<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100]  D_loss: 0.9882  G_loss: 0.8695\n",
      "Epoch 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:52<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100]  D_loss: 1.1168  G_loss: 0.8732\n",
      "Epoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:51<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100]  D_loss: 1.0000  G_loss: 0.8783\n",
      "Epoch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:53<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100]  D_loss: 1.0558  G_loss: 0.8831\n",
      "Epoch 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:53<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100]  D_loss: 1.0602  G_loss: 0.8877\n",
      "Epoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:56<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100]  D_loss: 1.1235  G_loss: 0.8935\n",
      "Epoch 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:57<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100]  D_loss: 0.9973  G_loss: 0.8987\n",
      "Epoch 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:55<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100]  D_loss: 1.0342  G_loss: 0.9042\n",
      "Epoch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:56<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/100]  D_loss: 1.0556  G_loss: 0.9099\n",
      "Epoch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:56<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100]  D_loss: 1.0549  G_loss: 0.9139\n",
      "Epoch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:55<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/100]  D_loss: 0.9859  G_loss: 0.9210\n",
      "Epoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:56<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100]  D_loss: 1.0339  G_loss: 0.9295\n",
      "Epoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:57<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100]  D_loss: 0.8871  G_loss: 0.9344\n",
      "Epoch 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:56<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100]  D_loss: 1.1072  G_loss: 0.9418\n",
      "Epoch 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:55<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100]  D_loss: 0.9138  G_loss: 0.9481\n",
      "Epoch 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:57<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100]  D_loss: 1.0907  G_loss: 0.9568\n",
      "Epoch 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:57<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100]  D_loss: 1.0663  G_loss: 0.9623\n",
      "Epoch 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:57<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100]  D_loss: 0.9950  G_loss: 0.9700\n",
      "Epoch 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:57<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/100]  D_loss: 0.9014  G_loss: 0.9772\n",
      "Epoch 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:57<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100]  D_loss: 0.8688  G_loss: 0.9848\n",
      "Epoch 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:59<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100]  D_loss: 0.8984  G_loss: 0.9919\n",
      "Epoch 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:56<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100]  D_loss: 0.9458  G_loss: 1.0002\n",
      "Epoch 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:58<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/100]  D_loss: 0.9170  G_loss: 1.0071\n",
      "Epoch 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:59<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100]  D_loss: 0.8171  G_loss: 1.0157\n",
      "Epoch 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [01:02<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100]  D_loss: 0.8584  G_loss: 1.0236\n",
      "Epoch 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [01:11<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100]  D_loss: 0.9594  G_loss: 1.0290\n",
      "Epoch 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [01:10<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/100]  D_loss: 0.7813  G_loss: 1.0394\n",
      "Epoch 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [01:00<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100]  D_loss: 0.8865  G_loss: 1.0487\n",
      "Epoch 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:58<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/100]  D_loss: 0.8180  G_loss: 1.0564\n",
      "Epoch 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:59<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/100]  D_loss: 0.9001  G_loss: 1.0658\n",
      "Epoch 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [00:59<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/100]  D_loss: 0.7689  G_loss: 1.0736\n",
      "Epoch 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step: 100%|████████████████████████████| 32/32 [01:12<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/100]  D_loss: 0.7533  G_loss: 1.0800\n",
      "Epoch 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step:  81%|██████████████████████▊     | 26/32 [01:06<00:15,  2.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 19\u001b[0m\n\u001b[1;32m     10\u001b[0m theta \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mrandn(n, k, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     13\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(n, \u001b[38;5;241m20\u001b[39m),\n\u001b[1;32m     14\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLeakyReLU(\u001b[38;5;241m0.01\u001b[39m),\n\u001b[1;32m     15\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     16\u001b[0m     nn\u001b[38;5;241m.\u001b[39mSigmoid()\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrain_qgan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantum_generator_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantum_gen_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# theta is a torch.nn.Parameter with shape (n, k)\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator_update_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_shift_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_qubits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 42\u001b[0m, in \u001b[0;36mtrain_qgan\u001b[0;34m(quantum_generator_fn, generator_params, discriminator, dataloader, num_epochs, disc_lr, latent_dim, device, generator_update_fn)\u001b[0m\n\u001b[1;32m     38\u001b[0m     fake_preds \u001b[38;5;241m=\u001b[39m discriminator(fake_data)\n\u001b[1;32m     40\u001b[0m     gen_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mlog(fake_preds \u001b[38;5;241m+\u001b[39m epsilon))\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mgenerator_update_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]  D_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisc_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  G_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgen_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 26\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(theta, loss, noise, disc)\u001b[0m\n\u001b[1;32m     10\u001b[0m theta \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mrandn(n, k, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     13\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(n, \u001b[38;5;241m20\u001b[39m),\n\u001b[1;32m     14\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLeakyReLU(\u001b[38;5;241m0.01\u001b[39m),\n\u001b[1;32m     15\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     16\u001b[0m     nn\u001b[38;5;241m.\u001b[39mSigmoid()\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m train_qgan(\n\u001b[1;32m     20\u001b[0m     quantum_generator_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m noise, theta: quantum_gen_fn(\n\u001b[1;32m     21\u001b[0m         noise, theta, n\u001b[38;5;241m=\u001b[39mn, k\u001b[38;5;241m=\u001b[39mk, bounds\u001b[38;5;241m=\u001b[39mbounds, mu\u001b[38;5;241m=\u001b[39mmu, sigma\u001b[38;5;241m=\u001b[39msigma\n\u001b[1;32m     22\u001b[0m     ),\n\u001b[1;32m     23\u001b[0m     generator_params\u001b[38;5;241m=\u001b[39mtheta,  \u001b[38;5;66;03m# theta is a torch.nn.Parameter with shape (n, k)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     discriminator\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     25\u001b[0m     dataloader\u001b[38;5;241m=\u001b[39mtrain_loader(num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, feature_dim\u001b[38;5;241m=\u001b[39mn, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m),\n\u001b[0;32m---> 26\u001b[0m     generator_update_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m theta, loss, noise, disc: \u001b[43mparameter_shift_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_qubits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m )\n",
      "Cell \u001b[0;32mIn[26], line 19\u001b[0m, in \u001b[0;36mparameter_shift_update\u001b[0;34m(theta, noise_batch, discriminator, lr, n_qubits, k_layers, bounds, mu, sigma)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Generate samples using shifted parameters\u001b[39;00m\n\u001b[1;32m     18\u001b[0m g_plus \u001b[38;5;241m=\u001b[39m quantum_gen_fn(noise_batch, theta_plus, n_qubits, k_layers, bounds, mu, sigma)\n\u001b[0;32m---> 19\u001b[0m g_minus \u001b[38;5;241m=\u001b[39m \u001b[43mquantum_gen_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_minus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_qubits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Evaluate discriminator outputs\u001b[39;00m\n\u001b[1;32m     22\u001b[0m D_plus \u001b[38;5;241m=\u001b[39m discriminator(g_plus)\n",
      "Cell \u001b[0;32mIn[25], line 27\u001b[0m, in \u001b[0;36mquantum_gen_fn\u001b[0;34m(noise_batch, theta, n, k, bounds, mu, sigma)\u001b[0m\n\u001b[1;32m     24\u001b[0m     qc\u001b[38;5;241m.\u001b[39mmeasure_all()\n\u001b[1;32m     25\u001b[0m     circuits\u001b[38;5;241m.\u001b[39mappend(qc)\n\u001b[0;32m---> 27\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43msampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Extract probabilities\u001b[39;00m\n\u001b[1;32m     30\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/qiskit/primitives/primitive_job.py:51\u001b[0m, in \u001b[0;36mPrimitiveJob.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResultT:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_submitted()\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_future\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from qiskit import QuantumCircuit\n",
    "from tqdm import tqdm\n",
    "\n",
    "mu = 0.5\n",
    "sigma = 1\n",
    "bounds = [-np.pi, np.pi]\n",
    "n = 6\n",
    "k = 1\n",
    "theta = nn.Parameter(torch.randn(n, k, dtype=torch.float32, requires_grad=True))\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n, 20),\n",
    "    nn.LeakyReLU(0.01),\n",
    "    nn.Linear(20, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "train_qgan(\n",
    "    quantum_generator_fn=lambda noise, theta: quantum_gen_fn(\n",
    "        noise, theta, n=n, k=k, bounds=bounds, mu=mu, sigma=sigma\n",
    "    ),\n",
    "    generator_params=theta,  # theta is a torch.nn.Parameter with shape (n, k)\n",
    "    discriminator=model,\n",
    "    dataloader=train_loader(num_samples=1000, feature_dim=n, batch_size=32),\n",
    "    generator_update_fn=lambda theta, loss, noise, disc: parameter_shift_update(\n",
    "        theta, noise, disc, lr=0.01, n_qubits=n, k_layers=k, bounds=bounds, mu=mu, sigma=sigma\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c7162-0540-4416-a815-62e448d40a0d",
   "metadata": {},
   "source": [
    "### Errors encountered:\n",
    "Typos, qubit count too large (50 blew up the data to 8 PiB!!! o.o), needed to see progress, so adding a loading bar with tqdm\n",
    "\n",
    "#### More information on the data explosion!\n",
    "So basically with 50 qubits, and the encoding of the normal distribution, our data blows up by 2^50 points on a grid, which is 8 petabytes of memory for float64 values, an absolutely crazy amount. We might modify the normal distribution to allow ourself to still have the normal distribution but better fitted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qiskit (stable)",
   "language": "python",
   "name": "qiskit-stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
